# Sentiment Analysis

## Method Description
Firstly, I go with the approach of retrieving reviews from files with a separator ”[t]”. Since there are two files with no explicit separator “[t]”, ”Canon_PowerShot_SD500.txt" and “ipod.txt”, I treat these two files separately as each sentence in those files as a separate review rather than viewing a file as one big review. The reviews are already converted entirely to lowercase at this stage. For each review, there are reviews with only “[+]” and “[-]” without subjective ratings, I treat them as +1 and -1 ratings respectively. The set of stop words commonly used for english language, accessible through nltk.stopwords.words(“english”), was excluded from the list of tokens. After that, taking ratings from each review, the data frame is created with columns for sentiment and review, which review contains review. Since there are two labels, positive and negative, positive is swapped with 1, and negative with 0.

Preprocessing is applied to reviews as well, but with just removing punctuations and undesirable symbols. After that, Tokenizer from the keras library does all the heavy lifting. After tokenization, tokens turned into lists of sequences. When we train neural networks for NLP, we need sequences to be of the same size. pad_sequences from keras is used to make all the reviews the same length as the maximum sequence length, which is 250. According to the given dataset, along with my text processing, I decided to use 1000 as the maximum number of words to be used.

The most commonly used RNNs are LSTMs, and this model will be having a many-to-one relationship. The inputs are sequences of words, the output is one single label. The model is constructed using Tensorflow and Keras. 

After initializing a Sequential model, an embedding layer is put in, which stores one vector per word. The Sequential model allows us to build deep neural networks by stacking layers one on top of another. It converts the sequences of word indices into sequences of vectors when it is called. After that, the SpatialDropout1D layer with a rate of 0.2 is added. It drops entire 1D feature maps instead of individual elements. After that, the LSTM layer is added with a dimensionality of 100, with both dropout and recurrent dropout set to 0.2. The Dropout improves the performance and generalization of the model, which is used to fight overfitting. The LSTM is made sure to perform recurrent dropout as well. A Dense Layer with one unit and sigmoid activation since it is a binary classification problem. An optimizer is added to minimize the loss function, and I chose to use Adam as it converges rapidly and rectifies the vanishing learning rate and high variance. Since we’re building a binary classifier, the loss function to minimize is binary_crossentropy. For classification problems, we set the metrics as accuracy.

## Result Analysis
I split the dataset into 80% Training and 20% Testing. 10% of the training data is used as validation data. EarlyStopping callback is passed into the training loop, which training loop checks at the end of every epoch whether the validation loss is no longer decreasing, considering the min_delta of 0.0001 and patience of 3. Patience defines the number of epochs with no improvement. Higher epochs hurts the accuracy as the model starts memorizing the data. For 5-Fold Validation, I used the sklearn library, KFold API is used to perform 5-fold consecutive folds with shuffling. During each iteration of 5-Fold, I also printed out the classification matrix to analyze the model more thoroughly. During the experiment, the training data size is around 351-352 rows.

After conducting 5-Fold consecutive fold validation, the mean accuracy was around 68-73 percent with standard deviations from 0.05 - 2.00. From the classification matrices I saw, precision, recall, and F-1 score are skewed toward positive reviews. This is mainly due to the issue with the dataset being unbalanced as the ratio of positive to negative reviews is around 3:1. This is due to major voting classification where the class distribution of the training data is skewed. Positive reviews tend to dominate the prediction of negative reviews. There is also a chance that shuffling might uneven the class distribution of the training data which affected the mean accuracy.

In trials with different units of LSTM dimensionality output space such as 50, 100, 150, and 200, the fewer units lowered the accuracy, though greater than 100 did not make any difference at all. I chose 100 as the units dimensionality output space of LSTM. I used the batch size of 16 as a small batch size would make it easier to fit one batch worth of training data in memory while offering a regularizing effect and lower generalization error. Using 32 as a batch size decreases the mean accuracy by around 2%. The dropout/ recurrent dropout rate of 0.2 performs best on the validation set, while rates of 0.1 and 0.5 yield similar results. Increasing the dropout rate above 0.5 deteriorated the mean accuracy. For the validation, there is a threat where validation test data is too small and is less consistent if I set the percentage of validation too low. I tried with 0.06 which worsen the model.

I did try oversampling on negative reviews. The issue was that the model seemed to occur overfitting since the technique made exact copies of the negative reviews examples in the training set. I did not try using the augmentation, but I think it might help increase the performance of the model.
